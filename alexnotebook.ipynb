{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1791ce9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stream_id</th>\n",
       "      <th>streamer_username</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_stop</th>\n",
       "      <th>streamer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11586</td>\n",
       "      <td>33827617344</td>\n",
       "      <td>miltontpike1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827755632</td>\n",
       "      <td>rekinss</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827475024</td>\n",
       "      <td>airon29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827351664</td>\n",
       "      <td>tonytubo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827169440</td>\n",
       "      <td>eliasmerk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051728</th>\n",
       "      <td>8975</td>\n",
       "      <td>34415693328</td>\n",
       "      <td>purple_hs</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051729</th>\n",
       "      <td>29709</td>\n",
       "      <td>34414041536</td>\n",
       "      <td>forsen</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051730</th>\n",
       "      <td>41485</td>\n",
       "      <td>34416038384</td>\n",
       "      <td>rekkles</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051731</th>\n",
       "      <td>84280</td>\n",
       "      <td>34413422016</td>\n",
       "      <td>dlxowns45</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051732</th>\n",
       "      <td>74685</td>\n",
       "      <td>34415760240</td>\n",
       "      <td>lvpes2</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>1648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3051733 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    stream_id streamer_username  time_start  time_stop  \\\n",
       "0          11586  33827617344      miltontpike1           0          5   \n",
       "1          13762  33827755632           rekinss           0          1   \n",
       "2          13762  33827475024           airon29           0          1   \n",
       "3          13762  33827351664          tonytubo           0          1   \n",
       "4          13762  33827169440         eliasmerk           0          1   \n",
       "...          ...          ...               ...         ...        ...   \n",
       "3051728     8975  34415693328         purple_hs        6147       6148   \n",
       "3051729    29709  34414041536            forsen        6147       6148   \n",
       "3051730    41485  34416038384           rekkles        6147       6148   \n",
       "3051731    84280  34413422016         dlxowns45        6147       6148   \n",
       "3051732    74685  34415760240            lvpes2        6147       6148   \n",
       "\n",
       "         streamer_id  \n",
       "0               1866  \n",
       "1               6845  \n",
       "2              18105  \n",
       "3               4949  \n",
       "4              47618  \n",
       "...              ...  \n",
       "3051728          727  \n",
       "3051729          202  \n",
       "3051730         2524  \n",
       "3051731         2190  \n",
       "3051732         1648  \n",
       "\n",
       "[3051733 rows x 6 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "data = pd.read_csv('100k_a.csv', names=['user_id', 'stream_id', 'streamer_username', 'time_start', 'time_stop'])\n",
    "# Create train and test splits temporally sorted by time_start\n",
    "data['streamer_id'], uniques = pd.factorize(data['streamer_username'])\n",
    "# start indexing at 0 instead of 1\n",
    "data['user_id'] = data['user_id'] - 1\n",
    "data = data.sort_values('time_start').reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "07b59c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stream_id</th>\n",
       "      <th>streamer_username</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_stop</th>\n",
       "      <th>streamer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2925064</th>\n",
       "      <td>67470</td>\n",
       "      <td>34395278048</td>\n",
       "      <td>shrimp9710</td>\n",
       "      <td>5898</td>\n",
       "      <td>5899</td>\n",
       "      <td>2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925076</th>\n",
       "      <td>5530</td>\n",
       "      <td>34394569824</td>\n",
       "      <td>enchatin</td>\n",
       "      <td>5898</td>\n",
       "      <td>5900</td>\n",
       "      <td>21835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925085</th>\n",
       "      <td>73313</td>\n",
       "      <td>34394440032</td>\n",
       "      <td>jukes</td>\n",
       "      <td>5898</td>\n",
       "      <td>5901</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925087</th>\n",
       "      <td>37797</td>\n",
       "      <td>34392164880</td>\n",
       "      <td>couragejd</td>\n",
       "      <td>5898</td>\n",
       "      <td>5900</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925104</th>\n",
       "      <td>8423</td>\n",
       "      <td>34393587008</td>\n",
       "      <td>trumpsc</td>\n",
       "      <td>5898</td>\n",
       "      <td>5900</td>\n",
       "      <td>6344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051728</th>\n",
       "      <td>8975</td>\n",
       "      <td>34415693328</td>\n",
       "      <td>purple_hs</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051729</th>\n",
       "      <td>29709</td>\n",
       "      <td>34414041536</td>\n",
       "      <td>forsen</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051730</th>\n",
       "      <td>41485</td>\n",
       "      <td>34416038384</td>\n",
       "      <td>rekkles</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051731</th>\n",
       "      <td>84280</td>\n",
       "      <td>34413422016</td>\n",
       "      <td>dlxowns45</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051732</th>\n",
       "      <td>74685</td>\n",
       "      <td>34415760240</td>\n",
       "      <td>lvpes2</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>1648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44334 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    stream_id streamer_username  time_start  time_stop  \\\n",
       "2925064    67470  34395278048        shrimp9710        5898       5899   \n",
       "2925076     5530  34394569824          enchatin        5898       5900   \n",
       "2925085    73313  34394440032             jukes        5898       5901   \n",
       "2925087    37797  34392164880         couragejd        5898       5900   \n",
       "2925104     8423  34393587008           trumpsc        5898       5900   \n",
       "...          ...          ...               ...         ...        ...   \n",
       "3051728     8975  34415693328         purple_hs        6147       6148   \n",
       "3051729    29709  34414041536            forsen        6147       6148   \n",
       "3051730    41485  34416038384           rekkles        6147       6148   \n",
       "3051731    84280  34413422016         dlxowns45        6147       6148   \n",
       "3051732    74685  34415760240            lvpes2        6147       6148   \n",
       "\n",
       "         streamer_id  \n",
       "2925064         2189  \n",
       "2925076        21835  \n",
       "2925085          117  \n",
       "2925087          103  \n",
       "2925104         6344  \n",
       "...              ...  \n",
       "3051728          727  \n",
       "3051729          202  \n",
       "3051730         2524  \n",
       "3051731         2190  \n",
       "3051732         1648  \n",
       "\n",
       "[44334 rows x 6 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many of the last rounds to keep?\n",
    "split_point = 250\n",
    "\n",
    "# Get the last 250 unique start times for the test set\n",
    "start_times = data['time_start'].unique()\n",
    "last_start_times = start_times[-split_point:]\n",
    "test_data = data[data['time_start'].isin(last_start_times)]\n",
    "\n",
    "# Keep the beginning start times for training\n",
    "train_data = data[~data['time_start'].isin(last_start_times)]\n",
    "\n",
    "assert len(train_data) + len(test_data) == len(data)\n",
    "\n",
    "# Filter to keep only the last interaction per user (largest time_start)\n",
    "test_data = test_data.drop_duplicates(subset=['user_id'], keep='last')\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4ebcb082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159263, 99822)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemIDs = train_data['streamer_id'].unique().tolist()\n",
    "userIDs = train_data['user_id'].unique().tolist()\n",
    "len(itemIDs), len(userIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c4fc7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairs of (user_id, streamer_id) in the training data\n",
    "trainInteractions = list(zip(train_data['user_id'], train_data['streamer_id']))\n",
    "# For each user id, this gets the set of consumed item ids (streamers they watched)\n",
    "userInteractions = train_data.groupby('user_id')['streamer_id'].apply(set).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6fc9077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT DELETE THIS CODE BLOCK ###\n",
    "num_users = int(train_data['user_id'].max()) + 1\n",
    "num_items = int(train_data['streamer_id'].max()) + 1\n",
    "\n",
    "\n",
    "class MFModel(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(MFModel, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(tf.random.normal([num_items],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([num_users, K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([num_items, K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i):\n",
    "        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "    \n",
    "    def recommend(self, u, N=10, availItems=None, userTime = None):\n",
    "        # Use average user vector if u is invalid\n",
    "        num_users = tf.shape(self.gammaU)[0]\n",
    "        if u >= num_users or u < 0:\n",
    "            # Use average user vector\n",
    "            gamma_u = tf.reduce_mean(self.gammaU, axis=0)\n",
    "        else:\n",
    "            gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "\n",
    "        # Compute dot product: (Items x K) . (K x 1) -> (Items x 1)\n",
    "        interaction_scores = tf.matmul(self.gammaI, tf.expand_dims(gamma_u, axis=-1))\n",
    "        \n",
    "        # Squeeze to (Items,) so it matches betaI shape\n",
    "        interaction_scores = tf.squeeze(interaction_scores, axis=-1)\n",
    "        \n",
    "        # Now shapes match: (Items,) + (Items,)\n",
    "        scores = self.betaI + interaction_scores\n",
    "        \n",
    "        if availItems is not None and userTime is not None:\n",
    "            available_mask = tf.constant(availItems, dtype=tf.int64)\n",
    "            scores = tf.gather(scores, available_mask)\n",
    "            # Note: top_N will now be from available items only\n",
    "            top_N = tf.math.top_k(scores, k=min(N, len(availItems)))\n",
    "            # Map back to original item indices\n",
    "            top_indices = tf.gather(availItems, top_N.indices)\n",
    "            return top_indices.numpy(), top_N.values.numpy()\n",
    "        else:\n",
    "            top_N = tf.math.top_k(scores, k=N)\n",
    "            return top_N.indices.numpy(), top_N.values.numpy()\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaU) +\\\n",
    "                            tf.nn.l2_loss(self.gammaI))\n",
    "    \n",
    "    def score(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int64)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int64)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "55810ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Negative sampling function with controlled repetition\n",
    "def sample_negative(u, i, user_consumed_train, all_items, Prepeat=0.5):\n",
    "    # Repeat negative sampling with probability Prepeat\n",
    "    if np.random.rand() < Prepeat:\n",
    "        consumed = list(user_consumed_train[u])\n",
    "        negItem = random.choice(consumed)\n",
    "        while negItem == i and len(consumed) > 1:\n",
    "            negItem = random.choice(consumed)\n",
    "    else:\n",
    "        negItem = random.choice(all_items)\n",
    "        while negItem == i and len(all_items) > 1:\n",
    "            negItem = random.choice(all_items)\n",
    "\n",
    "    return negItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ce29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.6930116\n",
      "iteration 20, objective = 0.693044\n",
      "iteration 30, objective = 0.6928355\n",
      "iteration 40, objective = 0.69252014\n",
      "iteration 50, objective = 0.692701\n",
      "iteration 60, objective = 0.69229233\n",
      "iteration 70, objective = 0.6918272\n",
      "iteration 80, objective = 0.69226575\n",
      "iteration 90, objective = 0.692529\n",
      "iteration 100, objective = 0.6920325\n",
      "iteration 110, objective = 0.6919879\n",
      "iteration 120, objective = 0.69125396\n",
      "iteration 130, objective = 0.6911702\n",
      "iteration 140, objective = 0.69068205\n",
      "iteration 150, objective = 0.69074553\n",
      "iteration 160, objective = 0.69118565\n",
      "iteration 170, objective = 0.6903305\n",
      "iteration 180, objective = 0.6905602\n",
      "iteration 190, objective = 0.69029963\n",
      "iteration 200, objective = 0.69038504\n",
      "iteration 210, objective = 0.68919474\n",
      "iteration 220, objective = 0.68979913\n",
      "iteration 230, objective = 0.6913753\n",
      "iteration 240, objective = 0.6886865\n",
      "iteration 250, objective = 0.6891781\n",
      "iteration 260, objective = 0.68960863\n",
      "iteration 270, objective = 0.6896234\n",
      "iteration 280, objective = 0.68951726\n",
      "iteration 290, objective = 0.6874103\n",
      "iteration 300, objective = 0.6911212\n",
      "iteration 310, objective = 0.6893669\n",
      "iteration 320, objective = 0.6908821\n",
      "iteration 330, objective = 0.6886923\n",
      "iteration 340, objective = 0.69066906\n",
      "iteration 350, objective = 0.6886276\n",
      "iteration 360, objective = 0.6875547\n",
      "iteration 370, objective = 0.6890208\n",
      "iteration 380, objective = 0.6869177\n",
      "iteration 390, objective = 0.68859327\n",
      "iteration 400, objective = 0.6846598\n",
      "iteration 410, objective = 0.6868259\n",
      "iteration 420, objective = 0.68934876\n",
      "iteration 430, objective = 0.6899561\n",
      "iteration 440, objective = 0.68649495\n",
      "iteration 450, objective = 0.686329\n",
      "iteration 460, objective = 0.6864295\n",
      "iteration 470, objective = 0.684729\n",
      "iteration 480, objective = 0.6858865\n",
      "iteration 490, objective = 0.6880762\n",
      "iteration 500, objective = 0.68452966\n",
      "iteration 510, objective = 0.6881389\n",
      "iteration 520, objective = 0.6853566\n",
      "iteration 530, objective = 0.6871893\n",
      "iteration 540, objective = 0.68333346\n",
      "iteration 550, objective = 0.68452924\n",
      "iteration 560, objective = 0.68641496\n",
      "iteration 570, objective = 0.6826689\n",
      "iteration 580, objective = 0.68634564\n",
      "iteration 590, objective = 0.6861695\n",
      "iteration 600, objective = 0.6841792\n",
      "iteration 610, objective = 0.68678737\n",
      "iteration 620, objective = 0.6865103\n",
      "iteration 630, objective = 0.6852972\n",
      "iteration 640, objective = 0.68116057\n",
      "iteration 650, objective = 0.6852536\n",
      "iteration 660, objective = 0.68833995\n",
      "iteration 670, objective = 0.6827271\n",
      "iteration 680, objective = 0.6845768\n",
      "iteration 690, objective = 0.683965\n",
      "iteration 700, objective = 0.68015546\n",
      "iteration 710, objective = 0.6834205\n",
      "iteration 720, objective = 0.6828456\n",
      "iteration 730, objective = 0.6852515\n",
      "iteration 740, objective = 0.68298084\n",
      "iteration 750, objective = 0.6890435\n",
      "iteration 760, objective = 0.67999166\n",
      "iteration 770, objective = 0.6826611\n",
      "iteration 780, objective = 0.68202406\n",
      "iteration 790, objective = 0.6807665\n",
      "iteration 800, objective = 0.6847335\n",
      "iteration 810, objective = 0.6845991\n",
      "iteration 820, objective = 0.6873092\n",
      "iteration 830, objective = 0.68907684\n",
      "iteration 840, objective = 0.6786176\n",
      "iteration 850, objective = 0.6757832\n",
      "iteration 860, objective = 0.6844553\n",
      "iteration 870, objective = 0.6807434\n",
      "iteration 880, objective = 0.682427\n",
      "iteration 890, objective = 0.6833659\n",
      "iteration 900, objective = 0.68357074\n",
      "iteration 910, objective = 0.678275\n",
      "iteration 920, objective = 0.6745453\n",
      "iteration 930, objective = 0.68256855\n",
      "iteration 940, objective = 0.68595225\n",
      "iteration 950, objective = 0.6759042\n",
      "iteration 960, objective = 0.68366987\n",
      "iteration 970, objective = 0.67574054\n",
      "iteration 980, objective = 0.6863671\n",
      "iteration 990, objective = 0.6760662\n",
      "iteration 1000, objective = 0.6804046\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import random\n",
    "model = MFModel(20, 0.00001)\n",
    "optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "\n",
    "def trainingStep(model, interactions):\n",
    "    Nsamples = 2**8\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleJ = [], [], []\n",
    "        for c in range(Nsamples):\n",
    "            u,i = random.choice(interactions) # positive sample\n",
    "            j = sample_negative(u, i, userInteractions, itemIDs, Prepeat=0.5)\n",
    "            sampleU.append(u)\n",
    "            sampleI.append(i)\n",
    "            sampleJ.append(j)\n",
    "\n",
    "        loss = model.call(sampleU,sampleI,sampleJ)\n",
    "        loss += model.reg()\n",
    "    \n",
    "    gradients = tape.gradient(loss, [model.betaI, model.gammaU, model.gammaI])\n",
    "    optimizer.apply_gradients(zip(gradients, [model.betaI, model.gammaU, model.gammaI]))\n",
    "\n",
    "    return loss.numpy()\n",
    "\n",
    "for i in range(1000):\n",
    "    obj = trainingStep(model, trainInteractions)\n",
    "    if (i+1) % 100 == 0: print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ece322cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44334"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SAVE THE MODEL ###\\\n",
    "# After training, save embeddings\n",
    "np.save('betaI.npy', model.betaI.numpy())\n",
    "np.save('gammaU.npy', model.gammaU.numpy())\n",
    "np.save('gammaI.npy', model.gammaI.numpy())\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2750653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new Python file or cell, recreate model with same sizes\n",
    "load_model = MFModel(K=20, lamb=LAMB)  # Same K and lamb, but lamb not needed for inference\n",
    "# Assign saved embeddings\n",
    "load_model.betaI.assign(np.load('betaI.npy'))\n",
    "load_model.gammaU.assign(np.load('gammaU.npy'))\n",
    "load_model.gammaI.assign(np.load('gammaI.npy'))\n",
    "# Now use load_model.score() or load_model.predict() for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "da4866f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = data[['streamer_id', 'time_start']].drop_duplicates()\n",
    "availMap = tempdf.groupby('time_start')['streamer_id'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "42835e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 test interactions.\n",
      "Processed 2000 test interactions.\n",
      "Processed 3000 test interactions.\n",
      "Processed 4000 test interactions.\n",
      "Processed 5000 test interactions.\n",
      "Processed 6000 test interactions.\n",
      "Processed 7000 test interactions.\n",
      "Processed 8000 test interactions.\n",
      "Processed 9000 test interactions.\n",
      "Processed 10000 test interactions.\n",
      "Processed 11000 test interactions.\n",
      "Processed 12000 test interactions.\n",
      "Processed 13000 test interactions.\n",
      "Processed 14000 test interactions.\n",
      "Processed 15000 test interactions.\n",
      "Processed 16000 test interactions.\n",
      "Processed 17000 test interactions.\n",
      "Processed 18000 test interactions.\n",
      "Processed 19000 test interactions.\n",
      "Processed 20000 test interactions.\n",
      "Processed 21000 test interactions.\n",
      "Processed 22000 test interactions.\n",
      "Processed 23000 test interactions.\n",
      "Processed 24000 test interactions.\n",
      "Processed 25000 test interactions.\n",
      "Processed 26000 test interactions.\n",
      "Processed 27000 test interactions.\n",
      "Processed 28000 test interactions.\n",
      "Processed 29000 test interactions.\n",
      "Processed 30000 test interactions.\n",
      "Processed 31000 test interactions.\n",
      "Processed 32000 test interactions.\n",
      "Processed 33000 test interactions.\n",
      "Processed 34000 test interactions.\n",
      "Processed 35000 test interactions.\n",
      "Processed 36000 test interactions.\n",
      "Processed 37000 test interactions.\n",
      "Processed 38000 test interactions.\n",
      "Processed 39000 test interactions.\n",
      "Processed 40000 test interactions.\n",
      "Processed 41000 test interactions.\n",
      "Processed 42000 test interactions.\n",
      "Processed 43000 test interactions.\n",
      "Processed 44000 test interactions.\n",
      "Hit@1 prediction accuracy for novel interactions: 0.020135566188197767\n",
      "Hit@1 prediction accuracy for repeat interactions: 0.03400942429829953\n",
      "Hit@1 prediction accuracy total 0.029300311273514686\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model with hit@1\n",
    "test_pairs = list(zip(test_data['user_id'], test_data['streamer_id'], test_data['time_start']))\n",
    "test_pairs = test_pairs  # limit to first 1000 for faster evaluation\n",
    "\n",
    "specific_value = 161\n",
    "filtered_rows = data[data['streamer_id'] == specific_value]\n",
    "# print(filtered_rows)\n",
    "\n",
    "hitkNovel = 0\n",
    "hitkNovelTotal = 0\n",
    "hitkRepeat = 0\n",
    "hitkRepeatTotal = 0\n",
    "\n",
    "i = 0\n",
    "trainInteractionsSet = set(trainInteractions)\n",
    "for uid, iid, user_time in test_pairs:\n",
    "    ## get available streamers at that time\n",
    "    availItems = availMap[user_time]\n",
    "\n",
    "    topItems, score = model.recommend(uid, N=1, availItems=availItems, userTime=user_time)\n",
    "    # print('Top recommended item for user', uid, 'is', topItem)\n",
    "    isHit = iid in topItems\n",
    "\n",
    "    if (uid, iid) in trainInteractionsSet:\n",
    "      if isHit:\n",
    "        hitkRepeat += 1\n",
    "      hitkRepeatTotal += 1\n",
    "    else:\n",
    "      if isHit:\n",
    "        hitkNovel += 1\n",
    "      hitkNovelTotal += 1\n",
    "\n",
    "    if (i+1) % 1000 == 0:\n",
    "      print(\"Processed \" + str(i+1) + \" test interactions.\")\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print('Hit@1 prediction accuracy for novel interactions:', hitkNovel / hitkNovelTotal)\n",
    "print('Hit@1 prediction accuracy for repeat interactions:', hitkRepeat / hitkRepeatTotal)\n",
    "print('Hit@1 prediction accuracy total', (hitkRepeat + hitkNovel) / (hitkRepeatTotal + hitkNovelTotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048a269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
