{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1791ce9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>stream_id</th>\n",
       "      <th>streamer_username</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_stop</th>\n",
       "      <th>streamer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11586</td>\n",
       "      <td>33827617344</td>\n",
       "      <td>miltontpike1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827755632</td>\n",
       "      <td>rekinss</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827475024</td>\n",
       "      <td>airon29</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827351664</td>\n",
       "      <td>tonytubo</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13762</td>\n",
       "      <td>33827169440</td>\n",
       "      <td>eliasmerk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>47618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051728</th>\n",
       "      <td>8975</td>\n",
       "      <td>34415693328</td>\n",
       "      <td>purple_hs</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051729</th>\n",
       "      <td>29709</td>\n",
       "      <td>34414041536</td>\n",
       "      <td>forsen</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051730</th>\n",
       "      <td>41485</td>\n",
       "      <td>34416038384</td>\n",
       "      <td>rekkles</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051731</th>\n",
       "      <td>84280</td>\n",
       "      <td>34413422016</td>\n",
       "      <td>dlxowns45</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>2190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051732</th>\n",
       "      <td>74685</td>\n",
       "      <td>34415760240</td>\n",
       "      <td>lvpes2</td>\n",
       "      <td>6147</td>\n",
       "      <td>6148</td>\n",
       "      <td>1648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3051733 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id    stream_id streamer_username  time_start  time_stop  \\\n",
       "0          11586  33827617344      miltontpike1           0          5   \n",
       "1          13762  33827755632           rekinss           0          1   \n",
       "2          13762  33827475024           airon29           0          1   \n",
       "3          13762  33827351664          tonytubo           0          1   \n",
       "4          13762  33827169440         eliasmerk           0          1   \n",
       "...          ...          ...               ...         ...        ...   \n",
       "3051728     8975  34415693328         purple_hs        6147       6148   \n",
       "3051729    29709  34414041536            forsen        6147       6148   \n",
       "3051730    41485  34416038384           rekkles        6147       6148   \n",
       "3051731    84280  34413422016         dlxowns45        6147       6148   \n",
       "3051732    74685  34415760240            lvpes2        6147       6148   \n",
       "\n",
       "         streamer_id  \n",
       "0               1866  \n",
       "1               6845  \n",
       "2              18105  \n",
       "3               4949  \n",
       "4              47618  \n",
       "...              ...  \n",
       "3051728          727  \n",
       "3051729          202  \n",
       "3051730         2524  \n",
       "3051731         2190  \n",
       "3051732         1648  \n",
       "\n",
       "[3051733 rows x 6 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "data = pd.read_csv('100k_a.csv', names=['user_id', 'stream_id', 'streamer_username', 'time_start', 'time_stop'])\n",
    "# Create train and test splits temporally sorted by time_start\n",
    "data['streamer_id'], uniques = pd.factorize(data['streamer_username'])\n",
    "# start indexing at 0 instead of 1\n",
    "data['user_id'] = data['user_id'] - 1\n",
    "data = data.sort_values('time_start').reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33f54f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2441386, 610347)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_point = int(len(data) * 0.8)\n",
    "# split_point = 100000 # for reproducibility\n",
    "shuffled_data = data.sample(frac=1, random_state=42).reset_index()\n",
    "\n",
    "train_data = data.iloc[:split_point]\n",
    "test_data = data.iloc[split_point:]\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ebcb082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98184"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemIDs = train_data['streamer_id'].unique().tolist()\n",
    "userIDs = train_data['user_id'].unique().tolist()\n",
    "len(userIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c4fc7915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2441386"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pairs of (user_id, streamer_id) in the training data\n",
    "trainInteractions = list(zip(train_data['user_id'], train_data['streamer_id']))\n",
    "# For each user id, this gets the set of consumed item ids (streamers they watched)\n",
    "user_consumed_items = train_data.groupby('user_id')['streamer_id'].apply(set).to_dict()\n",
    "len(trainInteractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6fc9077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT DELETE THIS CODE BLOCK ###\n",
    "num_users = int(data['user_id'].max()) + 1\n",
    "num_items = int(data['streamer_id'].max()) + 1\n",
    "\n",
    "class MFModel(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(MFModel, self).__init__()\n",
    "        # Initialize variables\n",
    "        # self.betaI = tf.Variable(tf.random.normal([num_items],stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([num_users, K],stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([num_items, K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i):\n",
    "        p = tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "    \n",
    "    def recommend(self, u, N=10):\n",
    "        u = tf.convert_to_tensor(u, dtype=tf.int64)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        \n",
    "        # Compute dot product: (Items x K) . (K x 1) -> (Items x 1)\n",
    "        interaction_scores = tf.matmul(self.gammaI, tf.expand_dims(gamma_u, axis=-1))\n",
    "        \n",
    "        # Squeeze to (Items,) so it matches betaI shape\n",
    "        scores = tf.squeeze(interaction_scores, axis=-1)\n",
    "        \n",
    "        top_N = tf.math.top_k(scores, k=N)\n",
    "        return top_N.indices.numpy(), top_N.values.numpy()\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.gammaU) +\\\n",
    "                            tf.nn.l2_loss(self.gammaI))\n",
    "    \n",
    "    def score(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int64)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int64)\n",
    "        # beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        x_ui = tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return x_ui\n",
    "\n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c18ce29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Objective=0.6938749551773071\n",
      "Epoch 2: Objective=0.6935511827468872\n",
      "Epoch 3: Objective=0.6935590505599976\n",
      "Epoch 4: Objective=0.6935566663742065\n",
      "Epoch 5: Objective=0.6935558915138245\n"
     ]
    }
   ],
   "source": [
    "# Train the model with batch\n",
    "LAMB = 0.01\n",
    "LR = 0.005\n",
    "model = MFModel(20, LAMB)\n",
    "optimizer = tf.keras.optimizers.Adam(LR)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "interactionsArr = np.array(trainInteractions)\n",
    "\n",
    "def samplePositiveBatch(interactionsArr, batch_size):\n",
    "    indices = np.random.choice(len(interactionsArr), batch_size)\n",
    "    batch_pos = interactionsArr[indices]\n",
    "    sampleU = batch_pos[:,0]\n",
    "    sampleI = batch_pos[:,1]\n",
    "\n",
    "    return sampleU, sampleI\n",
    "\n",
    "def sampleNegativeBatch(sampleU, sampleI, user_consumed_arrays, num_items, Prepeat=0.5):\n",
    "    batch_size = len(sampleU)\n",
    "    sampleJ = np.zeros(batch_size, dtype=np.int64)\n",
    "\n",
    "    for k in range(batch_size):\n",
    "        u = sampleU[k]\n",
    "        i = sampleI[k]\n",
    "        \n",
    "        # Logic: Try to sample from history (Repeat)\n",
    "        if np.random.rand() < Prepeat:\n",
    "            consumed = user_consumed_arrays[u]\n",
    "            \n",
    "            # We need a consumed item that is NOT 'i'.\n",
    "            # If the user has watched more than just 'i', we can proceed.\n",
    "            if len(consumed) > 1 or (len(consumed) == 1 and consumed[0] != i):\n",
    "                j = np.random.choice(consumed)\n",
    "                while j == i: # Rejection sampling (fast)\n",
    "                    j = np.random.choice(consumed)\n",
    "                sampleJ[k] = j\n",
    "                continue\n",
    "            # If user only watched 'i', fall through to random sampling below\n",
    "        \n",
    "        # Logic: Sample from all items (Novel)\n",
    "        j = np.random.randint(0, num_items)\n",
    "        while j == i: # Rejection sampling (fast)\n",
    "            j = np.random.randint(0, num_items)\n",
    "        sampleJ[k] = j\n",
    "\n",
    "    return sampleJ\n",
    "\n",
    "@tf.function\n",
    "def trainStepBatch(modela, sampleU, sampleI, sampleJ):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = modela.call(sampleU, sampleI, sampleJ)\n",
    "        loss += modela.reg()\n",
    "    \n",
    "    gradients = tape.gradient(loss, [modela.gammaU, modela.gammaI])\n",
    "    optimizer.apply_gradients(zip(gradients, [modela.gammaU, modela.gammaI]))\n",
    "    return loss\n",
    "\n",
    "BATCH_SIZE = 2**11 # 2^12=4096\n",
    "NUM_BATCHES = int(len(interactionsArr) / BATCH_SIZE)\n",
    "\n",
    "# Convert user_consumed_items to arrays for new negative sampling function\n",
    "user_consumed_arrays = {}\n",
    "for u in user_consumed_items:\n",
    "    user_consumed_arrays[u] = np.array(list(user_consumed_items[u]), dtype=np.int64)\n",
    "\n",
    "for epoch in range(5):\n",
    "    for i in range(NUM_BATCHES):\n",
    "        sampleU, sampleI = samplePositiveBatch(interactionsArr, BATCH_SIZE)\n",
    "        sampleJ = sampleNegativeBatch(sampleU, sampleI, user_consumed_arrays, num_items, Prepeat=0.5)\n",
    "        loss = trainStepBatch(model, sampleU, sampleI, sampleJ)\n",
    "        # if (i+1) % 1000 == 0:\n",
    "        #    print(f\"At batch {i+1}, Batch Objective={loss.numpy()}\")\n",
    "    print(f\"Epoch {epoch+1}: Objective={loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ece322cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE THE MODEL ###\\\n",
    "# After training, save embeddings\n",
    "# np.save('betaI.npy', model.betaI.numpy())\n",
    "np.save('gammaU.npy', model.gammaU.numpy())\n",
    "np.save('gammaI.npy', model.gammaI.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d2750653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(162625, 20) dtype=float32, numpy=\n",
       "array([[-7.17396470e-05, -5.34340739e-04, -3.60592094e-05, ...,\n",
       "        -1.00638630e-04, -2.09767953e-04, -3.54786171e-04],\n",
       "       [-2.54171435e-04, -4.29039181e-04,  2.56293133e-05, ...,\n",
       "         5.77376341e-05,  1.41092998e-04, -1.17051546e-04],\n",
       "       [-6.98132208e-05,  2.09647624e-04, -3.74293537e-04, ...,\n",
       "        -1.90546474e-04, -7.48530438e-05,  4.20468918e-04],\n",
       "       ...,\n",
       "       [ 4.39826144e-05,  1.02381862e-04, -1.28363166e-03, ...,\n",
       "         1.06336130e-03, -1.07350468e-06, -1.30806220e-04],\n",
       "       [-6.12991716e-05, -7.39211100e-06, -6.94112678e-05, ...,\n",
       "        -1.93974502e-05,  1.60652853e-05,  1.26920524e-03],\n",
       "       [ 3.14172780e-07,  4.66058264e-05,  5.51851525e-04, ...,\n",
       "        -4.18477575e-05, -3.41405348e-06, -9.31105751e-06]],\n",
       "      shape=(162625, 20), dtype=float32)>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a new Python file or cell, recreate model with same sizes\n",
    "load_model = MFModel(K=20, lamb=LAMB)  # Same K and lamb, but lamb not needed for inference\n",
    "# Assign saved embeddings\n",
    "# load_model.betaI.assign(np.load('betaI.npy'))\n",
    "load_model.gammaU.assign(np.load('gammaU.npy'))\n",
    "load_model.gammaI.assign(np.load('gammaI.npy'))\n",
    "# Now use load_model.score() or load_model.predict() for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42835e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on 100000 pairs...\n",
      "Hit@1 prediction accuracy for novel interactions: 0.0\n",
      "Hit@1 prediction accuracy for repeat interactions: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model with hit@1 (BATCHED)\n",
    "test_pairs = list(zip(test_data['user_id'], test_data['streamer_id']))\n",
    "test_pairs = test_pairs\n",
    "\n",
    "\n",
    "# CRITICAL OPTIMIZATION: Convert list to set for O(1) lookup\n",
    "trainInteractionsSet = set(trainInteractions)\n",
    "\n",
    "hit1Novel = 0\n",
    "hit1NovelTotal = 0\n",
    "hit1Repeat = 0\n",
    "hit1RepeatTotal = 0\n",
    "\n",
    "# Create batches\n",
    "EVAL_BATCH_SIZE = 2**11\n",
    "num_test = len(test_pairs)\n",
    "\n",
    "print(f\"Starting evaluation on {num_test} pairs...\")\n",
    "\n",
    "for i in range(0, num_test, EVAL_BATCH_SIZE):\n",
    "    batch = test_pairs[i : i + EVAL_BATCH_SIZE]\n",
    "    u_batch = [p[0] for p in batch]\n",
    "    i_batch = [p[1] for p in batch]\n",
    "    \n",
    "    # Get embeddings for this batch of users: (BatchSize, K)\n",
    "    u_tensor = tf.convert_to_tensor(u_batch, dtype=tf.int64)\n",
    "    batch_gamma_u = tf.nn.embedding_lookup(model.gammaU, u_tensor)\n",
    "    \n",
    "    # Compute scores for ALL items for these users: (BatchSize, K) x (K, NumItems) -> (BatchSize, NumItems)\n",
    "    # Note: Transpose gammaI to be (K, NumItems)\n",
    "    batch_scores = tf.matmul(batch_gamma_u, model.gammaI, transpose_b=True)\n",
    "    \n",
    "    # Get top 1 for everyone in the batch\n",
    "    top_indices = tf.math.argmax(batch_scores, axis=1).numpy()\n",
    "    \n",
    "    # Check hits\n",
    "    for k in range(len(batch)):\n",
    "        uid, iid = batch[k]\n",
    "        pred_item = top_indices[k]\n",
    "        \n",
    "        if (uid, iid) in trainInteractionsSet: # Note: This check is still slow if trainInteractions is a list. Make it a set!\n",
    "            if pred_item == iid:\n",
    "                hit1Repeat += 1\n",
    "            hit1RepeatTotal += 1\n",
    "        else:\n",
    "            if pred_item == iid:\n",
    "                hit1Novel += 1\n",
    "            hit1NovelTotal += 1\n",
    "            \n",
    "    if (i + EVAL_BATCH_SIZE) % 1000 == 0:\n",
    "        print(f\"Evaluated {i + EVAL_BATCH_SIZE}...\")\n",
    "\n",
    "print('Hit@1 prediction accuracy for novel interactions:', hit1Novel / (hit1NovelTotal + 1e-9))\n",
    "print('Hit@1 prediction accuracy for repeat interactions:', hit1Repeat / (hit1RepeatTotal + 1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048a269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
