{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Twitch Recommender Baselines: ALS vs. REP\n",
        "\n",
        "This notebook benchmarks two baseline models on the Twitch dataset:\n",
        "1. **ALS (Alternating Least Squares):** A matrix factorization model suited for implicit feedback.\n",
        "2. **REP (Repeat/Popularity):** A heuristic baseline that predicts re-watching (high on Twitch) and global popularity.\n",
        "\n",
        "**Evaluation Strategy:**\n",
        "We evaluate on two subsets of the test data to highlight the difference between \"Retention\" and \"Discovery\":\n",
        "- **Full Test Set:** Includes re-watches. (REP usually wins here).\n",
        "- **New Discovery Only:** Filters out any streamer the user has watched in training. (ALS should win here).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Loading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Optional: Try to import implicit for ALS. If not installed, we will skip ALS or warn.\n",
        "try:\n",
        "    import implicit\n",
        "    HAS_IMPLICIT = True\n",
        "except ImportError:\n",
        "    print(\"WARNING: 'implicit' library not found. Please run `pip install implicit`.\")\n",
        "    print(\"ALS steps will be skipped in this run.\")\n",
        "    HAS_IMPLICIT = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading & Preprocessing\n",
        "We load the CSV, calculate implicit weights (Duration), and Map IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwitchDataLoader:\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        self.user_map = {}\n",
        "        self.item_map = {}\n",
        "        self.item_map_inv = {}\n",
        "        \n",
        "    def load_and_process(self):\n",
        "        # Assuming no header based on description, but let's safely load\n",
        "        # Columns: User ID, Stream ID, Streamer username, Time start, Time stop\n",
        "        if not os.path.exists(self.filepath):\n",
        "            raise FileNotFoundError(f\"File {self.filepath} not found. Please ensure the CSV is in the directory.\")\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_csv(self.filepath, names=['user_id', 'stream_id', 'streamer', 'start', 'stop'])\n",
        "        except:\n",
        "            # Fallback if header exists\n",
        "            self.df = pd.read_csv(self.filepath)\n",
        "        \n",
        "        # 1. Calculate Duration (Implicit Feedback Strength)\n",
        "        self.df['duration'] = self.df['stop'] - self.df['start']\n",
        "        \n",
        "        # 2. Map Streamer Usernames to IDs\n",
        "        unique_streamers = self.df['streamer'].unique()\n",
        "        self.item_map = {name: i for i, name in enumerate(unique_streamers)}\n",
        "        self.item_map_inv = {i: name for i, name in enumerate(unique_streamers)}\n",
        "        self.df['item_idx'] = self.df['streamer'].map(self.item_map)\n",
        "        \n",
        "        # 3. Map User IDs to 0...N range\n",
        "        unique_users = self.df['user_id'].unique()\n",
        "        self.user_map = {uid: i for i, uid in enumerate(unique_users)}\n",
        "        self.df['user_idx'] = self.df['user_id'].map(self.user_map)\n",
        "        \n",
        "        print(f\"Loaded {len(self.df)} interactions.\")\n",
        "        print(f\"Users: {len(self.user_map)}, Streamers: {len(self.item_map)}\")\n",
        "        return self.df\n",
        "\n",
        "loader = TwitchDataLoader('100k_a.csv')\n",
        "full_df = loader.load_and_process()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Temporal Split (Train / Test)\n",
        "We split based on sorting by time and taking the first N% of rows.\n",
        "- **Train:** First 90% of interactions (sorted by time).\n",
        "- **Test:** Last 10% of interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def temporal_split(df, split_ratio=0.9):\n",
        "    # Sort by start time to ensure temporal order\n",
        "    df = df.sort_values(by='start')\n",
        "    \n",
        "    # Split by row count index\n",
        "    split_index = int(len(df) * split_ratio)\n",
        "    \n",
        "    print(f\"Splitting data at row {split_index} out of {len(df)}\")\n",
        "    \n",
        "    train_df = df.iloc[:split_index].copy()\n",
        "    test_df = df.iloc[split_index:].copy()\n",
        "    \n",
        "    # Filter Test: Only keep users who exist in Train (Cold start users are a different problem)\n",
        "    # train_users = set(train_df['user_idx'].unique())\n",
        "    # test_df = test_df[test_df['user_idx'].isin(train_users)]\n",
        "    \n",
        "    print(f\"Train samples: {len(train_df)}\")\n",
        "    print(f\"Test samples:  {len(test_df)}\")\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = temporal_split(full_df)\n",
        "\n",
        "# Create Sparse Matrices for ALS\n",
        "# Row = User, Col = Item\n",
        "def create_sparse_matrix(df, num_users, num_items):\n",
        "    # Sum duration if multiple interactions exist for same user-item\n",
        "    grouped = df.groupby(['user_idx', 'item_idx'])['duration'].sum().reset_index()\n",
        "    \n",
        "    sparse_mat = sparse.csr_matrix(\n",
        "        (grouped['duration'], (grouped['user_idx'], grouped['item_idx'])),\n",
        "        shape=(num_users, num_items)\n",
        "    )\n",
        "    return sparse_mat\n",
        "\n",
        "num_users = len(loader.user_map)\n",
        "num_items = len(loader.item_map)\n",
        "\n",
        "train_matrix = create_sparse_matrix(train_df, num_users, num_items)\n",
        "print(\"Sparse matrices created.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_user_concentration(matrix, user_idx):\n",
        "    \"\"\"\n",
        "    Calculates a concentration score (0.0 to 1.0) for a user's watch history.\n",
        "    \n",
        "    Logic: Uses the Herfindahl-Hirschman Index (HHI) on watch duration ratios.\n",
        "    Score = sum((duration_i / total_duration)^2)\n",
        "    \"\"\"\n",
        "    # Get the row for the user from the sparse matrix\n",
        "    # user_row is a sparse vector (1 x n_items)\n",
        "    user_row = matrix[user_idx]\n",
        "    \n",
        "    # Extract the non-zero values (durations)\n",
        "    durations = user_row.data\n",
        "    \n",
        "    if len(durations) == 0:\n",
        "        return 0.0 # No history, can be interpreted as 0 diversity or undefined. \n",
        "        \n",
        "    total_duration = np.sum(durations)\n",
        "    \n",
        "    if total_duration == 0:\n",
        "        return 0.0\n",
        "        \n",
        "    # Calculate ratios\n",
        "    ratios = durations / total_duration\n",
        "    \n",
        "    # Sum of 'squared' ratios - we've chosen to decrese this power to bring the mean closer to 0.5\n",
        "    score = np.sum(ratios ** 1.5)\n",
        "    \n",
        "    k=5\n",
        "    # k is the \"steepness\". k > 1 spreads values out.\n",
        "    # Center input at 0 by subtracting 0.5, multiply by gain, then sigmoid\n",
        "    return 1 / (1 + np.exp(-k * (score - 0.5)))\n",
        "    #return float(score)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate concentration scores for all users\n",
        "concentration_scores = [calculate_user_concentration(train_matrix, user_idx) for user_idx in range(num_users)]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(concentration_scores, bins=50, color='skyblue', edgecolor='black')\n",
        "plt.title('User Concentration Scores (HHI) Histogram')\n",
        "plt.xlabel('Concentration Score')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Exploratory Analysis\n",
        "Some intuition-building tests run on the dataset of 100k users to help realize how popular the most popular streamers are, and how common it is for a user to watch a new streamer. Hopefullly this can be a guide for how often our reccomender system should try to reccomend a streamer the user does not have a history of viewing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Top ten most watched streamers in train data\n",
        "\n",
        "unique_users_per_streamer = (\n",
        "    train_df.groupby(\"streamer\")[\"user_id\"]\n",
        "      .nunique()\n",
        "      .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unique_users_per_streamer.head(10).plot(kind='bar', figsize=(12, 6))\n",
        "\n",
        "plt.title(\"Top 10 Streamers by Unique Users\")\n",
        "plt.xlabel(\"Streamer Username\")\n",
        "plt.ylabel(\"Unique Users\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count novel user-streamer pairs in random sample of test data\n",
        "test_sample_size = 10_000\n",
        "tst_sample = test_df.sample(test_sample_size)\n",
        "#trn_sample = train_df.sample(frac=1/10000)\n",
        "\n",
        "# Build lookup set\n",
        "train_pairs = set(zip(train_df['user_id'], train_df['streamer']))\n",
        "\n",
        "novel_pairs = 0\n",
        "for row in tst_sample.itertuples(index=False):\n",
        "    if (row.user_id, row.streamer) not in train_pairs:\n",
        "        novel_pairs += 1\n",
        "\n",
        "print(f'Number of sampled test rows with novel user-streamer pairs: {novel_pairs} of {test_sample_size}')\n",
        "print(f'Percentage of novel pairs in test data: {novel_pairs / test_sample_size * 100:.2f}%')\n",
        "\n",
        "# Count the number of unique streamers in the dataset - gain intuition\n",
        "print('Number of unique streamers in entire dataset',full_df['streamer'].nunique())\n",
        "\n",
        "sorted = full_df.sort_values('start').reset_index(drop=True)\n",
        "user_watch_hist = {-1: {'example streamer'}}\n",
        "novel_instances = 0\n",
        "\n",
        "for row in sorted.itertuples(index=False):\n",
        "    if row.user_id not in user_watch_hist:\n",
        "        novel_instances += 1\n",
        "        user_watch_hist[row.user_id] = {row.streamer}\n",
        "    elif row.streamer not in user_watch_hist[row.user_id]:\n",
        "        novel_instances += 1\n",
        "        user_watch_hist[row.user_id].add(row.streamer)\n",
        "\n",
        "print('novel interactions (in entire dataset): ', novel_instances, '/', len(sorted), ' = ', novel_instances / len(sorted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation Engine\n",
        "We calculate Precision@K. We define two modes:\n",
        "1. **All Items:** Checks if recommendation exists in test set (Reward re-watching).\n",
        "2. **New Items:** Checks if recommendation exists in test set AND was NOT in train set (Reward discovery)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model_name, recommend_func, test_df, train_df, k=10, visualize=True):\n",
        "    print(f\"\\n--- Evaluating {model_name} @ K={k} ---\")\n",
        "    \n",
        "    # Pre-computation for visuals\n",
        "    test_user_items = test_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "    train_user_items = train_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "    \n",
        "    # Metrics\n",
        "    hits_all = 0\n",
        "    total_users = 0\n",
        "    hits_new = 0\n",
        "    total_users_with_new = 0\n",
        "    \n",
        "    # Data collectors for Visuals\n",
        "    all_recommendations = [] # For Popularity Bias\n",
        "    user_hit_status = [] # For Concentration Plot (conc_score, hit_binary)\n",
        "    \n",
        "    # Main Evaluation Loop\n",
        "    for u_idx, ground_truth_items in test_user_items.items():\n",
        "        recs = recommend_func(u_idx, k)\n",
        "        all_recommendations.extend(recs)\n",
        "        \n",
        "        # 1. Standard Metric\n",
        "        is_hit = 0\n",
        "        if len(set(recs) & ground_truth_items) > 0:\n",
        "            hits_all += 1\n",
        "            is_hit = 1\n",
        "        total_users += 1\n",
        "        \n",
        "        # Collect for Concentration Visual\n",
        "        if train_matrix is not None:\n",
        "            conc = calculate_user_concentration(train_matrix, u_idx)\n",
        "            user_hit_status.append((conc, is_hit))\n",
        "        \n",
        "        # 2. New Discovery Metric\n",
        "        past_items = train_user_items.get(u_idx, set())\n",
        "        true_new_items = ground_truth_items - past_items\n",
        "        \n",
        "        if len(true_new_items) > 0:\n",
        "            if len(set(recs) & true_new_items) > 0:\n",
        "                hits_new += 1\n",
        "            total_users_with_new += 1\n",
        "\n",
        "    precision_all = hits_all / total_users if total_users > 0 else 0\n",
        "    precision_new = hits_new / total_users_with_new if total_users_with_new > 0 else 0\n",
        "    \n",
        "    print(f\"Hit Rate (All Items): {precision_all:.4f}\")\n",
        "    print(f\"Hit Rate (New Only):  {precision_new:.4f}\")\n",
        "    \n",
        "    if visualize:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        fig.suptitle(f\"Visual Analysis: {model_name}\", fontsize=16)\n",
        "        \n",
        "        # --- Visual 1: Hit Rate @ K (Sensitivity) ---\n",
        "        k_values = [1, 5, 10, 20]\n",
        "        sens_scores = []\n",
        "        for kv in k_values:\n",
        "            # Lightweight calc for sensitivity\n",
        "            h = 0\n",
        "            t = 0\n",
        "            for u_idx, gt in list(test_user_items.items())[:500]: # Sample 500 for speed\n",
        "                r = recommend_func(u_idx, kv)\n",
        "                if len(set(r) & gt) > 0: h += 1\n",
        "                t += 1\n",
        "            sens_scores.append(h/t if t>0 else 0)\n",
        "            \n",
        "        axes[0].plot(k_values, sens_scores, marker='o', color='b')\n",
        "        axes[0].set_title(\"Sensitivity: Hit Rate @ K\")\n",
        "        axes[0].set_xlabel(\"K\")\n",
        "        axes[0].set_ylabel(\"Hit Rate\")\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # --- Visual 2: Popularity Bias ---\n",
        "        # Calculate training popularity ranks\n",
        "        pop_counts = train_df['item_idx'].value_counts().reset_index()\n",
        "        pop_counts.columns = ['item_idx', 'count']\n",
        "        pop_counts['rank'] = pop_counts['count'].rank(ascending=False)\n",
        "        rank_map = pop_counts.set_index('item_idx')['rank'].to_dict()\n",
        "        \n",
        "        rec_ranks = [rank_map.get(i, len(rank_map)+1) for i in all_recommendations]\n",
        "        \n",
        "        axes[1].hist(rec_ranks, bins=30, color='purple', alpha=0.7, log=True)\n",
        "        axes[1].set_title(\"Popularity Bias (Log Scale)\")\n",
        "        axes[1].set_xlabel(\"Streamer Popularity Rank (1=Top)\")\n",
        "        axes[1].set_ylabel(\"Freq of Recommendation\")\n",
        "        \n",
        "        # --- Visual 3: Performance vs Concentration ---\n",
        "        if user_hit_status:\n",
        "            x_vals = [x[0] for x in user_hit_status]\n",
        "            y_vals = [x[1] for x in user_hit_status]\n",
        "            # Add jitter to y_vals so points don't overlap completely on 0 and 1 lines\n",
        "            y_jitter = [y + np.random.normal(0, 0.05) for y in y_vals]\n",
        "            \n",
        "            axes[2].scatter(x_vals, y_jitter, alpha=0.3, c='teal', s=10)\n",
        "            axes[2].set_title(\"Performance vs User Focus\")\n",
        "            axes[2].set_xlabel(\"Concentration (0=Diverse, 1=Focus)\")\n",
        "            axes[2].set_yticks([0, 1])\n",
        "            axes[2].set_yticklabels(['Miss', 'Hit'])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return precision_all, precision_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Baseline Model: REP (Repeat / Popularity) and POP (Simple Popularity)\n",
        "These model represents the \"Naive\" Twitch strategy:\n",
        "1. Recommend what the user watched most in the past.\n",
        "2. If we need more items, fill with globally most popular streamers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class REPModel:\n",
        "    def __init__(self, train_df, num_items):\n",
        "        self.num_items = num_items\n",
        "        \n",
        "        # Precompute user favorites (sorted by total duration)\n",
        "        self.user_history = train_df.groupby('user_idx')['item_idx'].apply(\n",
        "            lambda x: x.value_counts().index.tolist()\n",
        "        ).to_dict()\n",
        "        \n",
        "        # Precompute global popularity\n",
        "        self.global_popular = train_df['item_idx'].value_counts().index.tolist()\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        recs = []\n",
        "        \n",
        "        # 1. Add History (Repeat)\n",
        "        if user_idx in self.user_history:\n",
        "            recs.extend(self.user_history[user_idx][:k])\n",
        "            \n",
        "        # 2. Fill with Popular (if needed)\n",
        "        if len(recs) < k:\n",
        "            for item in self.global_popular:\n",
        "                if item not in recs:\n",
        "                    recs.append(item)\n",
        "                    if len(recs) >= k:\n",
        "                        break\n",
        "        return recs[:k]\n",
        "\n",
        "print(\"Training REP Baseline...\")\n",
        "rep_model = REPModel(train_df, num_items)\n",
        "evaluate_model(\"REP (Repeat/Popularity) (k=1)\", rep_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"REP (Repeat/Popularity) (k=10)\", rep_model.recommend, test_df, train_df, k=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POPModel:\n",
        "    def __init__(self, train_df):\n",
        "        # Precompute global popularity\n",
        "        # value_counts() returns items sorted by frequency descending\n",
        "        self.global_popular = train_df['item_idx'].value_counts().index.tolist()\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        # Always return top k popular items\n",
        "        return self.global_popular[:k]\n",
        "\n",
        "print(\"Training POP Baseline...\")\n",
        "pop_model = POPModel(train_df)\n",
        "evaluate_model(\"POP (Global Popularity) (K=1)\", pop_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"POP (Global Popularity) (K=10)\", pop_model.recommend, test_df, train_df, k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Machine Learning Model: ALS (Implicit Matrix Factorization)\n",
        "We use the `implicit` library. This learns embeddings based on co-occurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if HAS_IMPLICIT:\n",
        "    print(\"\\nTraining ALS Model...\")\n",
        "    \n",
        "    # Implicit expects (items x users) usually, but AlternatingLeastSquares varies by version.\n",
        "    # Modern 'implicit' (0.5+) takes (users x items) in fit() usually, check version.\n",
        "    # We will use the standard setup: fit(user_item)\n",
        "    \n",
        "    # Initialize Model\n",
        "    # factors=64, regularization=0.05, iterations=20 are standard starting points\n",
        "    als_model = implicit.als.AlternatingLeastSquares(\n",
        "        factors=64, \n",
        "        regularization=0.05, \n",
        "        iterations=20,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    # Note: implicit expects (users, items) sparse matrix for training in recent versions\n",
        "    als_model.fit(train_matrix)\n",
        "    \n",
        "    def recommend_als(user_idx, k=10):\n",
        "        # implicit's recommend function\n",
        "        # filter_already_liked_items=False allows us to compare fairly with REP (which recommends history)\n",
        "        # However, for pure discovery, we might want True. \n",
        "        # We set False here to allow the model to decide if re-watching is relevant.\n",
        "        ids, scores = als_model.recommend(\n",
        "            user_idx, \n",
        "            train_matrix[user_idx], \n",
        "            N=k, \n",
        "            filter_already_liked_items=False \n",
        "        )\n",
        "        return ids\n",
        "    \n",
        "    evaluate_model(\"ALS (Matrix Factorization) (K=1)\", recommend_als, test_df, train_df, k=1)\n",
        "    evaluate_model(\"ALS (Matrix Factorization) (K = 10)\", recommend_als, test_df, train_df, k=10)\n",
        "\n",
        "    \n",
        "    # --- ALS (Pure Discovery Mode) ---\n",
        "    # Let's test ALS forced to explore (filter_already_liked_items=True)\n",
        "    def recommend_als_discovery(user_idx, k=10):\n",
        "        ids, scores = als_model.recommend(\n",
        "            user_idx, \n",
        "            train_matrix[user_idx], \n",
        "            N=k, \n",
        "            filter_already_liked_items=True \n",
        "        )\n",
        "        return ids\n",
        "        \n",
        "    evaluate_model(\"ALS (Discovery Mode) (K=1)\", recommend_als_discovery, test_df, train_df, k=1)\n",
        "    evaluate_model(\"ALS (Discovery Mode) (K=10)\", recommend_als_discovery, test_df, train_df, k=10)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Skipping ALS evaluation (library missing).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. MF-BPR With Uniform Negative Sampling\n",
        "Uses a preliminary model of matrix factorization with ranking criteria for from BPR. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "train_data = train_df\n",
        "test_data = test_df\n",
        "\n",
        "# Pairs of (user_id, streamer_id) in the training data\n",
        "trainInteractions = list(zip(train_data['user_idx'], train_data['item_idx']))\n",
        "# For each user id, this gets the set of consumed item ids (streamers they watched)\n",
        "user_consumed_items = train_data.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "len(trainInteractions)\n",
        "\n",
        "# Assuming 'data', 'trainInteractions', and 'user_consumed_items' exist from previous cells\n",
        "interactionsArr = np.array(trainInteractions)\n",
        "\n",
        "# --- 2. Negative Sampling Logic (Helper) ---\n",
        "def sampleNegativeBatch(sampleU, sampleI, num_items, Prepeat=0.5):\n",
        "    \"\"\"\n",
        "    Takes a batch of Users and Items, generates Negatives, and returns (U, I, J).\n",
        "    \"\"\"\n",
        "    batch_size = len(sampleU)\n",
        "    sampleJ = np.zeros(batch_size, dtype=np.int64)\n",
        "\n",
        "    for k in range(batch_size):\n",
        "        u = sampleU[k]\n",
        "        i = sampleI[k]\n",
        "\n",
        "        # Logic: Try to sample from history (Repeat)\n",
        "        if np.random.rand() < Prepeat:\n",
        "            consumed = user_consumed_arrays[u]\n",
        "            if len(consumed) > 1 or (len(consumed) == 1 and consumed[0] != i):\n",
        "                j = np.random.choice(consumed)\n",
        "                while j == i:\n",
        "                    j = np.random.choice(consumed)\n",
        "                sampleJ[k] = j\n",
        "                continue\n",
        "\n",
        "        # Logic: Novel Sample\n",
        "        j = np.random.randint(0, num_items)\n",
        "        while j == i:\n",
        "            j = np.random.randint(0, num_items)\n",
        "        sampleJ[k] = j\n",
        "\n",
        "    return sampleU, sampleI, sampleJ\n",
        "\n",
        "def negativeSampling(u_batch, i_batch):\n",
        "    # This wrapper allows the dataset pipeline to call our Python function\n",
        "    # It takes (U, I) and returns (U, I, J)\n",
        "    u_out, i_out, j_out = tf.numpy_function(\n",
        "        func=lambda u, i: sampleNegativeBatch(u, i, num_items, Prepeat=0.5),\n",
        "        inp=[u_batch, i_batch],\n",
        "        Tout=[tf.int64, tf.int64, tf.int64]\n",
        "    )\n",
        "    return u_out, i_out, j_out\n",
        "\n",
        "class MFModel(tf.keras.Model):\n",
        "    def __init__(self, K, lamb):\n",
        "        super(MFModel, self).__init__()\n",
        "        # Initialize with stddev=0.1 for better learning signal\n",
        "        self.betaI = tf.Variable(tf.random.normal([num_items],stddev=0.001))\n",
        "        self.gammaU = tf.Variable(tf.random.normal([num_users, K], stddev=0.1))\n",
        "        self.gammaI = tf.Variable(tf.random.normal([num_items, K], stddev=0.1))\n",
        "        self.lamb = lamb\n",
        "\n",
        "    def predict(self, u, i):\n",
        "        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
        "        return p\n",
        "\n",
        "    def recommend(self, u, N=10):\n",
        "        u = tf.convert_to_tensor(u, dtype=tf.int64)\n",
        "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
        "        interaction_scores = tf.matmul(self.gammaI, tf.expand_dims(gamma_u, axis=-1))\n",
        "        scores = self.betaI + tf.squeeze(interaction_scores, axis=-1)\n",
        "        top_N = tf.math.top_k(scores, k=N)\n",
        "        return top_N.indices.numpy(), top_N.values.numpy()\n",
        "\n",
        "    def reg(self):\n",
        "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\\\n",
        "                    tf.nn.l2_loss(self.gammaU) +\\\n",
        "                    tf.nn.l2_loss(self.gammaI))\n",
        "\n",
        "    def score(self, sampleU, sampleI):\n",
        "        u = tf.convert_to_tensor(sampleU, dtype=tf.int64)\n",
        "        i = tf.convert_to_tensor(sampleI, dtype=tf.int64)\n",
        "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
        "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
        "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
        "        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
        "        return x_ui\n",
        "\n",
        "    def call(self, inputs):\n",
        "        sampleU, sampleI, sampleJ = inputs\n",
        "        x_ui = self.score(sampleU, sampleI)\n",
        "        x_uj = self.score(sampleU, sampleJ)\n",
        "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))\n",
        "\n",
        "    # --- NEW: Custom Training Step for model.fit() ---\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data tuple (u, i, j) provided by the dataset\n",
        "        u, i, j = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.call((u, i, j))\n",
        "            loss += self.reg()\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, [self.betaI, self.gammaU, self.gammaI])\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, [self.betaI, self.gammaU, self.gammaI]))\n",
        "\n",
        "        # Return metrics\n",
        "        return {\"loss\": loss}\n",
        "    \n",
        "# --- 1. Data Preparation ---\n",
        "print(\"Converting user history to arrays...\")\n",
        "user_consumed_arrays = {}\n",
        "for u in user_consumed_items:\n",
        "    user_consumed_arrays[u] = np.array(list(user_consumed_items[u]), dtype=np.int64)\n",
        "\n",
        "# Prepare positive training data\n",
        "train_u = interactionsArr[:, 0]\n",
        "train_i = interactionsArr[:, 1]\n",
        "\n",
        "# --- 2. Create High-Performance Dataset ---\n",
        "BATCH_SIZE = 2**13  # 16,384\n",
        "\n",
        "# 1. Create dataset of (User, Item)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_u, train_i))\n",
        "dataset = dataset.shuffle(buffer_size=100000, reshuffle_each_iteration=True)\n",
        "\n",
        "# 2. Batch FIRST (so we process 16k items in Python at once, not 1 at a time)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# 3. Map (User, Item) -> (User, Item, NegativeItem)\n",
        "dataset = dataset.map(tf_negative_sampling, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# --- 3. Model Initialization ---\n",
        "LAMB = 0.0001\n",
        "LR = 0.0005\n",
        "EPOCHS = 10\n",
        "\n",
        "# Re-init model class (Use the corrected one from previous turn!)\n",
        "model = MFModel(K=20, lamb=LAMB)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(LR))\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "print(\"Starting training...\")\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n",
        "\n",
        "def MFRecommend(user_idx, k=10):\n",
        "    # implicit's recommend function\n",
        "    ids, scores = model.recommend(\n",
        "        user_idx, \n",
        "        k, \n",
        "    )\n",
        "    return ids\n",
        "evaluate_model(\"MF (BPR-NegSample) Model (K=1)\", MFRecommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"MF (BPR-NegSample) Model (K=10)\", MFRecommend, test_df, train_df, k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Hybrid Model (Stochastic Mix)\n",
        "Mixes REP (Retention) and ALS Discovery (Exploration).\n",
        "For each slot, flips a coin: >0.5 takes from REP, else from ALS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridRandomModel:\n",
        "    def __init__(self, rep_model, als_func):\n",
        "        self.rep_model = rep_model\n",
        "        self.als_func = als_func\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        # Fetch buffers from both sources (get k from each to ensure enough candidates)\n",
        "        rep_candidates = self.rep_model.recommend(user_idx, k=k)\n",
        "        als_candidates = self.als_func(user_idx, k=k)\n",
        "        \n",
        "        recs = []\n",
        "        seen = set()\n",
        "        \n",
        "        rep_ptr = 0\n",
        "        als_ptr = 0\n",
        "        \n",
        "        # Fill k slots\n",
        "        while len(recs) < k:\n",
        "            # If both exhausted, stop\n",
        "            if rep_ptr >= len(rep_candidates) and als_ptr >= len(als_candidates):\n",
        "                break\n",
        "                \n",
        "            choice = random.random()\n",
        "            use_rep = False\n",
        "            \n",
        "            # Decision Logic\n",
        "            if choice > 0.4:\n",
        "                if rep_ptr < len(rep_candidates):\n",
        "                    use_rep = True\n",
        "                else:\n",
        "                    use_rep = False # Fallback to ALS\n",
        "            else:\n",
        "                if als_ptr < len(als_candidates):\n",
        "                    use_rep = False\n",
        "                else:\n",
        "                    use_rep = True # Fallback to REP\n",
        "            \n",
        "            # Selection\n",
        "            item = None\n",
        "            if use_rep:\n",
        "                item = rep_candidates[rep_ptr]\n",
        "                rep_ptr += 1\n",
        "            else:\n",
        "                item = als_candidates[als_ptr]\n",
        "                als_ptr += 1\n",
        "            \n",
        "            # Deduplicate\n",
        "            if item not in seen:\n",
        "                recs.append(item)\n",
        "                seen.add(item)\n",
        "                \n",
        "        return recs\n",
        "\n",
        "hybrid_model = HybridRandomModel(rep_model, recommend_als_discovery)\n",
        "\n",
        "print(\"\\n--- Evaluating Hybrid Model (REP + ALS Discovery) ---\")\n",
        "evaluate_model(\"Hybrid Random (K=1)\", hybrid_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"Hybrid Random (K=10)\", hybrid_model.recommend, test_df, train_df, k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Hybrid Model with Diversity/Concentration Analysis Function\n",
        "This diversity function calculates a \"Concentration Score\" based on the Herfindahl-Hirschman Index (HHI).\n",
        "1.0 -> Only watches one streamer (High concentration)\n",
        "0.0 -> Watches many streamers equally (Low concentration)\n",
        "\n",
        "We then use this in the Hybrid Model to determine how often REP or ASL-discovery is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightedHybridModel:\n",
        "    def __init__(self, rep_model, als_func, train_matrix):\n",
        "        self.rep_model = rep_model\n",
        "        self.als_func = als_func\n",
        "        self.train_matrix = train_matrix\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        # Fetch buffers from both sources (get k from each to ensure enough candidates)\n",
        "        rep_candidates = self.rep_model.recommend(user_idx, k=k)\n",
        "        als_candidates = self.als_func(user_idx, k=k)\n",
        "        diversity_score = calculate_user_concentration(train_matrix, user_idx)\n",
        "        \n",
        "        recs = []\n",
        "        seen = set()\n",
        "        \n",
        "        rep_ptr = 0\n",
        "        als_ptr = 0\n",
        "        \n",
        "        # Fill k slots\n",
        "        while len(recs) < k:\n",
        "            # If both exhausted, stop\n",
        "            if rep_ptr >= len(rep_candidates) and als_ptr >= len(als_candidates):\n",
        "                break\n",
        "                \n",
        "            choice = random.random()\n",
        "            use_rep = False\n",
        "            \n",
        "            # Decision Logic\n",
        "            if choice < diversity_score:\n",
        "                if rep_ptr < len(rep_candidates):\n",
        "                    use_rep = True\n",
        "                else:\n",
        "                    use_rep = False # Fallback to ALS\n",
        "            else:\n",
        "                if als_ptr < len(als_candidates):\n",
        "                    use_rep = False\n",
        "                else:\n",
        "                    use_rep = True # Fallback to REP\n",
        "            \n",
        "            # Selection\n",
        "            item = None\n",
        "            if use_rep:\n",
        "                item = rep_candidates[rep_ptr]\n",
        "                rep_ptr += 1\n",
        "            else:\n",
        "                item = als_candidates[als_ptr]\n",
        "                als_ptr += 1\n",
        "            \n",
        "            # Deduplicate\n",
        "            if item not in seen:\n",
        "                recs.append(item)\n",
        "                seen.add(item)\n",
        "                \n",
        "        return recs\n",
        "\n",
        "w_hybrid_model = WeightedHybridModel(rep_model, recommend_als_discovery, train_matrix)\n",
        "\n",
        "print(\"\\n--- Evaluating WeightedHybridModel (REP + ALS Discovery) ---\")\n",
        "evaluate_model(\"Weighted Hybrid (K=1)\", w_hybrid_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"Weighted Hybrid (K=10)\", w_hybrid_model.recommend, test_df, train_df, k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary & Interpretation\n",
        "\n",
        "**Expected Results:**\n",
        "1. **Hit Rate (All Items):** **REP** should effectively tie or beat ALS. In our dataset, about 50% of consumption is re-watching. A model that simply says \"watch what you watched yesterday\" is incredibly hard to beat for general engagement.\n",
        "2. **Hit Rate (New Only):** **ALS (Discovery Mode)** should crush REP. REP relies on history; it fails to find *new* items (except via crude global popularity). ALS uses collaborative filtering (\"Users like you watched X\") to find specific, niche new streamers for the user.\n",
        "3. **Hybrid Model:** Should sit between REP and ALS, offering a balanced trade-off between retention (All Items Hit Rate) and exploration (New Items Hit Rate)."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
