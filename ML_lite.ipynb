{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Twitch Recommender Baselines: ALS vs. REP\n",
        "\n",
        "This notebook benchmarks two baseline models on the Twitch dataset:\n",
        "1. **ALS (Alternating Least Squares):** A matrix factorization model suited for implicit feedback.\n",
        "2. **REP (Repeat/Popularity):** A heuristic baseline that predicts re-watching (high on Twitch) and global popularity.\n",
        "\n",
        "**Evaluation Strategy:**\n",
        "We evaluate on two subsets of the test data to highlight the difference between \"Retention\" and \"Discovery\":\n",
        "- **Full Test Set:** Includes re-watches. (REP usually wins here).\n",
        "- **New Discovery Only:** Filters out any streamer the user has watched in training. (ALS should win here).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Loading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sparse\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "# Optional: Try to import implicit for ALS. If not installed, we will skip ALS or warn.\n",
        "try:\n",
        "    import implicit\n",
        "    HAS_IMPLICIT = True\n",
        "except ImportError:\n",
        "    print(\"WARNING: 'implicit' library not found. Please run `pip install implicit`.\")\n",
        "    print(\"ALS steps will be skipped in this run.\")\n",
        "    HAS_IMPLICIT = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading & Preprocessing\n",
        "We load the CSV, calculate implicit weights (Duration), and Map IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TwitchDataLoader:\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        self.user_map = {}\n",
        "        self.item_map = {}\n",
        "        self.item_map_inv = {}\n",
        "        \n",
        "    def load_and_process(self):\n",
        "        # Assuming no header based on description, but let's safely load\n",
        "        # Columns: User ID, Stream ID, Streamer username, Time start, Time stop\n",
        "        if not os.path.exists(self.filepath):\n",
        "            raise FileNotFoundError(f\"File {self.filepath} not found. Please ensure the CSV is in the directory.\")\n",
        "\n",
        "        try:\n",
        "            self.df = pd.read_csv(self.filepath, names=['user_id', 'stream_id', 'streamer', 'start', 'stop'])\n",
        "        except:\n",
        "            # Fallback if header exists\n",
        "            self.df = pd.read_csv(self.filepath)\n",
        "        \n",
        "        # 1. Calculate Duration (Implicit Feedback Strength)\n",
        "        self.df['duration'] = self.df['stop'] - self.df['start']\n",
        "        \n",
        "        # 2. Map Streamer Usernames to IDs\n",
        "        unique_streamers = self.df['streamer'].unique()\n",
        "        self.item_map = {name: i for i, name in enumerate(unique_streamers)}\n",
        "        self.item_map_inv = {i: name for i, name in enumerate(unique_streamers)}\n",
        "        self.df['item_idx'] = self.df['streamer'].map(self.item_map)\n",
        "        \n",
        "        # 3. Map User IDs to 0...N range\n",
        "        unique_users = self.df['user_id'].unique()\n",
        "        self.user_map = {uid: i for i, uid in enumerate(unique_users)}\n",
        "        self.df['user_idx'] = self.df['user_id'].map(self.user_map)\n",
        "        \n",
        "        print(f\"Loaded {len(self.df)} interactions.\")\n",
        "        print(f\"Users: {len(self.user_map)}, Streamers: {len(self.item_map)}\")\n",
        "        return self.df\n",
        "\n",
        "loader = TwitchDataLoader('100k_a.csv')\n",
        "full_df = loader.load_and_process()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Temporal Split (Train / Test)\n",
        "We split based on sorting by time and taking the first N% of rows.\n",
        "- **Train:** First 90% of interactions (sorted by time).\n",
        "- **Test:** Last 10% of interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def temporal_split(df, split_ratio=0.8):\n",
        "    # Sort by start time to ensure temporal order\n",
        "    df = df.sort_values(by='start')\n",
        "    \n",
        "    # Split by row count index\n",
        "    split_index = int(len(df) * split_ratio)\n",
        "    \n",
        "    print(f\"Splitting data at row {split_index} out of {len(df)}\")\n",
        "    \n",
        "    train_df = df.iloc[:split_index].copy()\n",
        "    test_df = df.iloc[split_index:].copy()\n",
        "    \n",
        "    # Filter Test: Only keep users who exist in Train (Cold start users are a different problem)\n",
        "    train_users = set(train_df['user_idx'].unique())\n",
        "    test_df = test_df[test_df['user_idx'].isin(train_users)]\n",
        "    \n",
        "    print(f\"Train samples: {len(train_df)}\")\n",
        "    print(f\"Test samples:  {len(test_df)}\")\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = temporal_split(full_df)\n",
        "\n",
        "# Create Sparse Matrices for ALS\n",
        "# Row = User, Col = Item\n",
        "def create_sparse_matrix(df, num_users, num_items):\n",
        "    # Sum duration if multiple interactions exist for same user-item\n",
        "    grouped = df.groupby(['user_idx', 'item_idx'])['duration'].sum().reset_index()\n",
        "    \n",
        "    sparse_mat = sparse.csr_matrix(\n",
        "        (grouped['duration'], (grouped['user_idx'], grouped['item_idx'])),\n",
        "        shape=(num_users, num_items)\n",
        "    )\n",
        "    return sparse_mat\n",
        "\n",
        "num_users = len(loader.user_map)\n",
        "num_items = len(loader.item_map)\n",
        "\n",
        "train_matrix = create_sparse_matrix(train_df, num_users, num_items)\n",
        "print(\"Sparse matrices created.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation Engine\n",
        "We calculate Precision@K. We define two modes:\n",
        "1. **All Items:** Checks if recommendation exists in test set (Reward re-watching).\n",
        "2. **New Items:** Checks if recommendation exists in test set AND was NOT in train set (Reward discovery)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model_name, recommend_func, test_df, train_df, k=10):\n",
        "    print(f\"\\n--- Evaluating {model_name} @ K={k} ---\")\n",
        "    \n",
        "    # Group test items by user\n",
        "    test_user_items = test_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "    \n",
        "    # Group train items (to filter for \"New\" metrics)\n",
        "    train_user_items = train_df.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "    \n",
        "    hits_all = 0\n",
        "    total_users = 0\n",
        "    hits_new = 0\n",
        "    total_users_with_new = 0\n",
        "    \n",
        "    for u_idx, ground_truth_items in test_user_items.items():\n",
        "        # Get recommendations\n",
        "        # recommend_func returns list of item_indices\n",
        "        recs = recommend_func(u_idx, k)\n",
        "        \n",
        "        # 1. Standard Metric (Retention + Discovery)\n",
        "        # Check intersection\n",
        "        if len(set(recs) & ground_truth_items) > 0:\n",
        "            hits_all += 1\n",
        "        total_users += 1\n",
        "        \n",
        "        # 2. New Discovery Metric\n",
        "        # Identify which ground truth items were actually \"new\" for this user\n",
        "        past_items = train_user_items.get(u_idx, set())\n",
        "        true_new_items = ground_truth_items - past_items\n",
        "        \n",
        "        if len(true_new_items) > 0:\n",
        "            # We only care if the model recommended one of these NEW items\n",
        "            # But the model might recommend old items. That's fine, we just check if it found a new one.\n",
        "            if len(set(recs) & true_new_items) > 0:\n",
        "                hits_new += 1\n",
        "            total_users_with_new += 1\n",
        "\n",
        "    precision_all = hits_all / total_users if total_users > 0 else 0\n",
        "    precision_new = hits_new / total_users_with_new if total_users_with_new > 0 else 0\n",
        "    \n",
        "    print(f\"Hit Rate (All Items): {precision_all:.4f} (User found *something* they liked)\")\n",
        "    print(f\"Hit Rate (New Only):  {precision_new:.4f} (User found *something new* they liked)\")\n",
        "    return precision_all, precision_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Baseline Model: REP (Repeat / Popularity)\n",
        "This model represents the \"Naive\" Twitch strategy:\n",
        "1. Recommend what the user watched most in the past.\n",
        "2. If we need more items, fill with globally most popular streamers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Most Popular Streamers in the Training Dataset:\n",
        "\n",
        "Based on the training set, we look at the streamer names with the most unique users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_users_per_streamer = (\n",
        "    train_df.groupby(\"streamer\")[\"user_id\"]\n",
        "      .nunique()\n",
        "      .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "unique_users_per_streamer.head(10).plot(kind='bar', figsize=(12, 6))\n",
        "\n",
        "plt.title(\"Top 10 Streamers by Unique Users\")\n",
        "plt.xlabel(\"Streamer Username\")\n",
        "plt.ylabel(\"Unique Users\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class REPModel:\n",
        "    def __init__(self, train_df, num_items):\n",
        "        self.num_items = num_items\n",
        "        \n",
        "        # Precompute user favorites (sorted by total duration)\n",
        "        self.user_history = train_df.groupby('user_idx')['item_idx'].apply(\n",
        "            lambda x: x.value_counts().index.tolist()\n",
        "        ).to_dict()\n",
        "        \n",
        "        # Precompute global popularity\n",
        "        self.global_popular = train_df['item_idx'].value_counts().index.tolist()\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        recs = []\n",
        "        \n",
        "        # 1. Add History (Repeat)\n",
        "        if user_idx in self.user_history:\n",
        "            recs.extend(self.user_history[user_idx][:k])\n",
        "            \n",
        "        # 2. Fill with Popular (if needed)\n",
        "        if len(recs) < k:\n",
        "            for item in self.global_popular:\n",
        "                if item not in recs:\n",
        "                    recs.append(item)\n",
        "                    if len(recs) >= k:\n",
        "                        break\n",
        "        return recs[:k]\n",
        "\n",
        "print(\"Training REP Baseline...\")\n",
        "rep_model = REPModel(train_df, num_items)\n",
        "evaluate_model(\"REP (Repeat/Popularity) (k=1)\", rep_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"REP (Repeat/Popularity) (k=10)\", rep_model.recommend, test_df, train_df, k=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class POPModel:\n",
        "    def __init__(self, train_df):\n",
        "        # Precompute global popularity\n",
        "        # value_counts() returns items sorted by frequency descending\n",
        "        self.global_popular = train_df['item_idx'].value_counts().index.tolist()\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        # Always return top k popular items\n",
        "        return self.global_popular[:k]\n",
        "\n",
        "print(\"Training POP Baseline...\")\n",
        "pop_model = POPModel(train_df)\n",
        "evaluate_model(\"POP (Global Popularity) (k=1)\", pop_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"POP (Global Popularity) (k=10)\", pop_model.recommend, test_df, train_df, k=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Machine Learning Model: ALS (Implicit Matrix Factorization)\n",
        "We use the `implicit` library. This learns embeddings based on co-occurrence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if HAS_IMPLICIT:\n",
        "    print(\"\\nTraining ALS Model...\")\n",
        "    \n",
        "    # Implicit expects (items x users) usually, but AlternatingLeastSquares varies by version.\n",
        "    # Modern 'implicit' (0.5+) takes (users x items) in fit() usually, check version.\n",
        "    # We will use the standard setup: fit(user_item)\n",
        "    \n",
        "    # Initialize Model\n",
        "    # factors=64, regularization=0.05, iterations=20 are standard starting points\n",
        "    als_model = implicit.als.AlternatingLeastSquares(\n",
        "        factors=64, \n",
        "        regularization=0.05, \n",
        "        iterations=20,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    # Note: implicit expects (users, items) sparse matrix for training in recent versions\n",
        "    als_model.fit(train_matrix)\n",
        "    \n",
        "    def recommend_als(user_idx, k=10):\n",
        "        # implicit's recommend function\n",
        "        # filter_already_liked_items=False allows us to compare fairly with REP (which recommends history)\n",
        "        # However, for pure discovery, we might want True. \n",
        "        # We set False here to allow the model to decide if re-watching is relevant.\n",
        "        ids, scores = als_model.recommend(\n",
        "            user_idx, \n",
        "            train_matrix[user_idx], \n",
        "            N=k, \n",
        "            filter_already_liked_items=False \n",
        "        )\n",
        "        return ids\n",
        "    \n",
        "    evaluate_model(\"ALS (Matrix Factorization) (K = 10)\", recommend_als, test_df, train_df, k=10)\n",
        "    evaluate_model(\"ALS (Matrix Factorization) (K=1)\", recommend_als, test_df, train_df, k=1)\n",
        "\n",
        "    \n",
        "    # --- ALS (Pure Discovery Mode) ---\n",
        "    # Let's test ALS forced to explore (filter_already_liked_items=True)\n",
        "    def recommend_als_discovery(user_idx, k=10):\n",
        "        ids, scores = als_model.recommend(\n",
        "            user_idx, \n",
        "            train_matrix[user_idx], \n",
        "            N=k, \n",
        "            filter_already_liked_items=True \n",
        "        )\n",
        "        return ids\n",
        "        \n",
        "    evaluate_model(\"ALS (Discovery Mode) (K=10)\", recommend_als_discovery, test_df, train_df, k=10)\n",
        "    evaluate_model(\"ALS (Discovery Mode) (K=1)\", recommend_als_discovery, test_df, train_df, k=1)\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Skipping ALS evaluation (library missing).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Hybrid Model (Stochastic Mix)\n",
        "Mixes REP (Retention) and ALS Discovery (Exploration).\n",
        "For each slot, flips a coin: >0.5 takes from REP, else from ALS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HybridRandomModel:\n",
        "    def __init__(self, rep_model, als_func):\n",
        "        self.rep_model = rep_model\n",
        "        self.als_func = als_func\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        # Fetch buffers from both sources (get k from each to ensure enough candidates)\n",
        "        rep_candidates = self.rep_model.recommend(user_idx, k=k)\n",
        "        als_candidates = self.als_func(user_idx, k=k)\n",
        "        \n",
        "        recs = []\n",
        "        seen = set()\n",
        "        \n",
        "        rep_ptr = 0\n",
        "        als_ptr = 0\n",
        "        \n",
        "        # Fill k slots\n",
        "        while len(recs) < k:\n",
        "            # If both exhausted, stop\n",
        "            if rep_ptr >= len(rep_candidates) and als_ptr >= len(als_candidates):\n",
        "                break\n",
        "                \n",
        "            choice = random.random()\n",
        "            use_rep = False\n",
        "            \n",
        "            # Decision Logic\n",
        "            if choice > 0.4:\n",
        "                if rep_ptr < len(rep_candidates):\n",
        "                    use_rep = True\n",
        "                else:\n",
        "                    use_rep = False # Fallback to ALS\n",
        "            else:\n",
        "                if als_ptr < len(als_candidates):\n",
        "                    use_rep = False\n",
        "                else:\n",
        "                    use_rep = True # Fallback to REP\n",
        "            \n",
        "            # Selection\n",
        "            item = None\n",
        "            if use_rep:\n",
        "                item = rep_candidates[rep_ptr]\n",
        "                rep_ptr += 1\n",
        "            else:\n",
        "                item = als_candidates[als_ptr]\n",
        "                als_ptr += 1\n",
        "            \n",
        "            # Deduplicate\n",
        "            if item not in seen:\n",
        "                recs.append(item)\n",
        "                seen.add(item)\n",
        "                \n",
        "        return recs\n",
        "\n",
        "hybrid_model = HybridRandomModel(rep_model, recommend_als_discovery)\n",
        "\n",
        "print(\"\\n--- Evaluating Hybrid Model (REP + ALS Discovery) ---\")\n",
        "evaluate_model(\"Hybrid Random (K=1)\", hybrid_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"Hybrid Random (K=10)\", hybrid_model.recommend, test_df, train_df, k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Hybrid Model with Diversity/Concentration Analysis Function\n",
        "This diversity function calculates a \"Concentration Score\" based on the Herfindahl-Hirschman Index (HHI).\n",
        "1.0 -> Only watches one streamer (High concentration)\n",
        "0.0 -> Watches many streamers equally (Low concentration)\n",
        "\n",
        "We then use this in the Hybrid Model to determine how often REP or ASL-discovery is used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_user_concentration(matrix, user_idx):\n",
        "    \"\"\"\n",
        "    Calculates a concentration score (0.0 to 1.0) for a user's watch history.\n",
        "    \n",
        "    Logic: Uses the Herfindahl-Hirschman Index (HHI) on watch duration ratios.\n",
        "    Score = sum((duration_i / total_duration)^2)\n",
        "    \"\"\"\n",
        "    # Get the row for the user from the sparse matrix\n",
        "    # user_row is a sparse vector (1 x n_items)\n",
        "    user_row = matrix[user_idx]\n",
        "    \n",
        "    # Extract the non-zero values (durations)\n",
        "    durations = user_row.data\n",
        "    \n",
        "    if len(durations) == 0:\n",
        "        return 0.0 # No history, can be interpreted as 0 diversity or undefined. \n",
        "        \n",
        "    total_duration = np.sum(durations)\n",
        "    \n",
        "    if total_duration == 0:\n",
        "        return 0.0\n",
        "        \n",
        "    # Calculate ratios\n",
        "    ratios = durations / total_duration\n",
        "    \n",
        "    # Sum of squared ratios\n",
        "    score = np.sum(ratios ** 2)\n",
        "    \n",
        "    return float(score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightedHybridModel:\n",
        "    def __init__(self, rep_model, als_func, train_matrix):\n",
        "        self.rep_model = rep_model\n",
        "        self.als_func = als_func\n",
        "        self.train_matrix = train_matrix\n",
        "        \n",
        "    def recommend(self, user_idx, k=10):\n",
        "        # Fetch buffers from both sources (get k from each to ensure enough candidates)\n",
        "        rep_candidates = self.rep_model.recommend(user_idx, k=k)\n",
        "        als_candidates = self.als_func(user_idx, k=k)\n",
        "        diversity_score = calculate_user_concentration(train_matrix, user_idx)\n",
        "        \n",
        "        recs = []\n",
        "        seen = set()\n",
        "        \n",
        "        rep_ptr = 0\n",
        "        als_ptr = 0\n",
        "        \n",
        "        # Fill k slots\n",
        "        while len(recs) < k:\n",
        "            # If both exhausted, stop\n",
        "            if rep_ptr >= len(rep_candidates) and als_ptr >= len(als_candidates):\n",
        "                break\n",
        "                \n",
        "            choice = random.random()\n",
        "            use_rep = False\n",
        "            \n",
        "            # Decision Logic\n",
        "            if choice < diversity_score:\n",
        "                if rep_ptr < len(rep_candidates):\n",
        "                    use_rep = True\n",
        "                else:\n",
        "                    use_rep = False # Fallback to ALS\n",
        "            else:\n",
        "                if als_ptr < len(als_candidates):\n",
        "                    use_rep = False\n",
        "                else:\n",
        "                    use_rep = True # Fallback to REP\n",
        "            \n",
        "            # Selection\n",
        "            item = None\n",
        "            if use_rep:\n",
        "                item = rep_candidates[rep_ptr]\n",
        "                rep_ptr += 1\n",
        "            else:\n",
        "                item = als_candidates[als_ptr]\n",
        "                als_ptr += 1\n",
        "            \n",
        "            # Deduplicate\n",
        "            if item not in seen:\n",
        "                recs.append(item)\n",
        "                seen.add(item)\n",
        "                \n",
        "        return recs\n",
        "\n",
        "w_hybrid_model = WeightedHybridModel(rep_model, recommend_als_discovery, train_matrix)\n",
        "\n",
        "print(\"\\n--- Evaluating WeightedHybridModel (REP + ALS Discovery) ---\")\n",
        "evaluate_model(\"Weighted Hybrid (K=1)\", w_hybrid_model.recommend, test_df, train_df, k=1)\n",
        "evaluate_model(\"Weighted Hybrid (K=10)\", w_hybrid_model.recommend, test_df, train_df, k=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary & Interpretation\n",
        "\n",
        "**Expected Results:**\n",
        "1. **Hit Rate (All Items):** **REP** should effectively tie or beat ALS. In our dataset, about 50% of consumption is re-watching. A model that simply says \"watch what you watched yesterday\" is incredibly hard to beat for general engagement.\n",
        "2. **Hit Rate (New Only):** **ALS (Discovery Mode)** should crush REP. REP relies on history; it fails to find *new* items (except via crude global popularity). ALS uses collaborative filtering (\"Users like you watched X\") to find specific, niche new streamers for the user.\n",
        "3. **Hybrid Model:** Should sit between REP and ALS, offering a balanced trade-off between retention (All Items Hit Rate) and exploration (New Items Hit Rate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
